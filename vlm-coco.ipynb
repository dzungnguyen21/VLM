{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-16T11:25:45.037023Z",
     "iopub.status.busy": "2026-01-16T11:25:45.036661Z",
     "iopub.status.idle": "2026-01-16T11:25:45.698723Z",
     "shell.execute_reply": "2026-01-16T11:25:45.698058Z",
     "shell.execute_reply.started": "2026-01-16T11:25:45.036992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-17 11:39:05--  https://github.com/RUCAIBox/POPE/blob/main/POPEv2/dataset/annotations.json\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘/vlm/annotations/annotations.json’\n",
      "\n",
      "annotations.json        [ <=>                ] 713.89K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2026-01-17 11:39:05 (5.50 MB/s) - ‘/vlm/annotations/annotations.json’ saved [731028]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!wget https://github.com/RUCAIBox/POPE/blob/main/POPEv2/dataset/annotations.json -P /vlm/annotations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:25:45.700760Z",
     "iopub.status.busy": "2026-01-16T11:25:45.700488Z",
     "iopub.status.idle": "2026-01-16T11:25:59.657507Z",
     "shell.execute_reply": "2026-01-16T11:25:59.656896Z",
     "shell.execute_reply.started": "2026-01-16T11:25:45.700734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from pycocotools.coco import COCO\n",
    "\n",
    "# coco = COCO(\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/annotations/instances_train2014.json\")\n",
    "\n",
    "# img_id = 123\n",
    "# ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "# anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "# objects = set()\n",
    "# for ann in anns:\n",
    "#     cat = coco.loadCats(ann[\"category_id\"])[0][\"name\"]\n",
    "#     objects.add(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:25:59.658639Z",
     "iopub.status.busy": "2026-01-16T11:25:59.658359Z",
     "iopub.status.idle": "2026-01-16T11:25:59.695006Z",
     "shell.execute_reply": "2026-01-16T11:25:59.694429Z",
     "shell.execute_reply.started": "2026-01-16T11:25:59.658613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "LLAVA_JSON = \"llava_instruct_150k.json\"\n",
    "\n",
    "COCO_DIRS = [\n",
    "    \"coco2014/images/train2014\",\n",
    "    \"coco2014/images/val2014\",\n",
    "    \"coco2014/images/test2014\",\n",
    "]\n",
    "\n",
    "OUTPUT_JSONL = \"My Drive/llava_train.jsonl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:25:59.696660Z",
     "iopub.status.busy": "2026-01-16T11:25:59.696448Z",
     "iopub.status.idle": "2026-01-16T11:25:59.701428Z",
     "shell.execute_reply": "2026-01-16T11:25:59.700867Z",
     "shell.execute_reply.started": "2026-01-16T11:25:59.696641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_img(img_name):\n",
    "    coco_fix = [\n",
    "        \"COCO_train2014_\",\n",
    "        \"COCO_val2014\",\n",
    "        \"COCO_tes2014\",\n",
    "    ]\n",
    "\n",
    "    for coco_dir in COCO_DIRS:\n",
    "        for prefix in coco_fix:\n",
    "            path = os.path.join(coco_dir, prefix + img_name)\n",
    "            if os.path.exists(path):\n",
    "                return path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:25:59.702336Z",
     "iopub.status.busy": "2026-01-16T11:25:59.702158Z",
     "iopub.status.idle": "2026-01-16T11:29:07.197995Z",
     "shell.execute_reply": "2026-01-16T11:29:07.197193Z",
     "shell.execute_reply.started": "2026-01-16T11:25:59.702319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with open(LLAVA_JSON, \"r\") as f:\n",
    "#     llava_data = json.load(f)\n",
    "\n",
    "# valid = 0\n",
    "# missing = 0\n",
    "\n",
    "# with open(OUTPUT_JSONL, \"w\") as out:\n",
    "#     for sample in tqdm(llava_data):\n",
    "#         image_name = sample.get(\"image\", \"\")\n",
    "#         image_path = find_img(image_name)\n",
    "\n",
    "#         if image_path is None:\n",
    "#             missing += 1\n",
    "#             continue\n",
    "\n",
    "#         record = {\n",
    "#             \"id\": sample.get(\"id\"),\n",
    "#             \"image\": image_path,\n",
    "#             \"conversations\": sample[\"conversations\"]\n",
    "#         }\n",
    "\n",
    "#         out.write(json.dumps(record) + \"\\n\")\n",
    "#         valid += 1\n",
    "\n",
    "# print(f\"Valid samples: {valid}\")\n",
    "# print(f\"Missing images: {missing}\")\n",
    "# print(f\"Saved to: {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T11:56:43.677510Z",
     "iopub.status.busy": "2026-01-16T11:56:43.676827Z",
     "iopub.status.idle": "2026-01-16T11:56:43.889876Z",
     "shell.execute_reply": "2026-01-16T11:56:43.889260Z",
     "shell.execute_reply.started": "2026-01-16T11:56:43.677479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'My Drive/llava_train.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-143136095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_JSONL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'My Drive/llava_train.jsonl'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with open(OUTPUT_JSONL) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sample = json.loads(random.choice(lines))\n",
    "print(sample[\"image\"])\n",
    "print(sample[\"conversations\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:00:12.378603Z",
     "iopub.status.busy": "2026-01-16T12:00:12.377794Z",
     "iopub.status.idle": "2026-01-16T12:02:52.353355Z",
     "shell.execute_reply": "2026-01-16T12:02:52.352257Z",
     "shell.execute_reply.started": "2026-01-16T12:00:12.378571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers accelerate bitsandbytes sentencepiece protobuf pillow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:03:23.688007Z",
     "iopub.status.busy": "2026-01-16T12:03:23.687260Z",
     "iopub.status.idle": "2026-01-16T12:03:23.701344Z",
     "shell.execute_reply": "2026-01-16T12:03:23.700637Z",
     "shell.execute_reply.started": "2026-01-16T12:03:23.687965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    CLIPVisionModel, CLIPImageProcessor,\n",
    "    LlamaForCausalLM, LlamaTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:04:39.843398Z",
     "iopub.status.busy": "2026-01-16T12:04:39.843010Z",
     "iopub.status.idle": "2026-01-16T12:04:39.864440Z",
     "shell.execute_reply": "2026-01-16T12:04:39.863517Z",
     "shell.execute_reply.started": "2026-01-16T12:04:39.843354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: LLaVA Model Architecture\n",
    "class LLaVAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_model_name=\"openai/clip-vit-large-patch14\",\n",
    "        llm_model_name=\"meta-llama/Llama-2-7b-chat-hf\",  # or use smaller model\n",
    "        projection_dim=4096\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Vision encoder (CLIP)\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(vision_model_name)\n",
    "        self.vision_processor = CLIPImageProcessor.from_pretrained(vision_model_name)\n",
    "        \n",
    "        # Freeze vision encoder\n",
    "        for param in self.vision_tower.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get vision feature dimension\n",
    "        vision_hidden_size = self.vision_tower.config.hidden_size  # 1024 for CLIP-L\n",
    "        \n",
    "        # Projection layer to map vision features to LLM dimension\n",
    "        self.mm_projector = nn.Sequential(\n",
    "            nn.Linear(vision_hidden_size, projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        # Language model\n",
    "        self.llm = LlamaForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            load_in_8bit=True  # Use 8-bit quantization to save memory\n",
    "        )\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(llm_model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        \n",
    "        # Special token for image\n",
    "        self.image_token_id = self.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "        \n",
    "    def encode_images(self, images):\n",
    "        \"\"\"Encode images using CLIP vision encoder\"\"\"\n",
    "        with torch.no_grad():\n",
    "            image_features = self.vision_tower(images).last_hidden_state\n",
    "        # Project to LLM dimension\n",
    "        image_features = self.mm_projector(image_features)\n",
    "        return image_features\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, images=None, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        input_ids: tokenized text with <image> placeholder\n",
    "        images: preprocessed images\n",
    "        labels: target tokens for training\n",
    "        \"\"\"\n",
    "        if images is not None:\n",
    "            # Encode images\n",
    "            image_features = self.encode_images(images)  # [B, 257, 4096]\n",
    "            \n",
    "            # Find positions of <image> tokens\n",
    "            batch_size = input_ids.shape[0]\n",
    "            new_input_embeds = []\n",
    "            new_labels = [] if labels is not None else None\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                # Get text embeddings\n",
    "                text_embeds = self.llm.model.embed_tokens(input_ids[i])\n",
    "                \n",
    "                # Find image token position\n",
    "                image_token_positions = (input_ids[i] == self.image_token_id).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(image_token_positions) > 0:\n",
    "                    # Replace <image> token with actual image features\n",
    "                    img_pos = image_token_positions[0]\n",
    "                    \n",
    "                    # Concatenate: text before image + image features + text after image\n",
    "                    new_embed = torch.cat([\n",
    "                        text_embeds[:img_pos],\n",
    "                        image_features[i],\n",
    "                        text_embeds[img_pos+1:]\n",
    "                    ], dim=0)\n",
    "                    \n",
    "                    new_input_embeds.append(new_embed)\n",
    "                    \n",
    "                    if labels is not None:\n",
    "                        # Adjust labels for image features\n",
    "                        new_label = torch.cat([\n",
    "                            labels[i][:img_pos],\n",
    "                            torch.full((image_features.shape[1],), -100, device=labels.device, dtype=labels.dtype),\n",
    "                            labels[i][img_pos+1:]\n",
    "                        ], dim=0)\n",
    "                        new_labels.append(new_label)\n",
    "                else:\n",
    "                    new_input_embeds.append(text_embeds)\n",
    "                    if labels is not None:\n",
    "                        new_labels.append(labels[i])\n",
    "            \n",
    "            # Pad sequences to same length\n",
    "            max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "            \n",
    "            padded_embeds = []\n",
    "            padded_labels = [] if labels is not None else None\n",
    "            new_attention_mask = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                embed = new_input_embeds[i]\n",
    "                pad_len = max_len - embed.shape[0]\n",
    "                \n",
    "                # Pad embeddings\n",
    "                padded_embed = torch.cat([\n",
    "                    embed,\n",
    "                    torch.zeros(pad_len, embed.shape[1], device=embed.device, dtype=embed.dtype)\n",
    "                ], dim=0)\n",
    "                padded_embeds.append(padded_embed)\n",
    "                \n",
    "                # Pad attention mask\n",
    "                attn_mask = torch.cat([\n",
    "                    torch.ones(embed.shape[0], device=device),\n",
    "                    torch.zeros(pad_len, device=device)\n",
    "                ], dim=0)\n",
    "                new_attention_mask.append(attn_mask)\n",
    "                \n",
    "                # Pad labels\n",
    "                if labels is not None:\n",
    "                    label = new_labels[i]\n",
    "                    padded_label = torch.cat([\n",
    "                        label,\n",
    "                        torch.full((pad_len,), -100, device=label.device, dtype=label.dtype)\n",
    "                    ], dim=0)\n",
    "                    padded_labels.append(padded_label)\n",
    "            \n",
    "            input_embeds = torch.stack(padded_embeds)\n",
    "            attention_mask = torch.stack(new_attention_mask)\n",
    "            labels = torch.stack(padded_labels) if labels is not None else None\n",
    "            \n",
    "            # Forward through LLM with embeddings\n",
    "            outputs = self.llm(\n",
    "                inputs_embeds=input_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                return_dict=True\n",
    "            )\n",
    "        else:\n",
    "            # Text-only forward\n",
    "            outputs = self.llm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                return_dict=True\n",
    "            )\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "print(\"Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:04:42.419473Z",
     "iopub.status.busy": "2026-01-16T12:04:42.419142Z",
     "iopub.status.idle": "2026-01-16T12:04:42.431671Z",
     "shell.execute_reply": "2026-01-16T12:04:42.430989Z",
     "shell.execute_reply.started": "2026-01-16T12:04:42.419443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataset Class\n",
    "class LLaVADataset(Dataset):\n",
    "    def __init__(self, jsonl_path, vision_processor, tokenizer, max_length=512):\n",
    "        self.data = []\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        \n",
    "        self.vision_processor = vision_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Add image token if not exists\n",
    "        if \"<image>\" not in self.tokenizer.get_vocab():\n",
    "            self.tokenizer.add_tokens([\"<image>\"])\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        image_path = item['image']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            pixel_values = self.vision_processor(images=image, return_tensors=\"pt\")['pixel_values'][0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return dummy image\n",
    "            pixel_values = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Format conversation\n",
    "        conversations = item['conversations']\n",
    "        \n",
    "        # Build conversation text\n",
    "        conversation_text = \"\"\n",
    "        for conv in conversations:\n",
    "            if conv['from'] == 'human':\n",
    "                # Replace <image> placeholder with special token\n",
    "                text = conv['value'].replace('<image>', '<image>')\n",
    "                conversation_text += f\"USER: {text}\\n\"\n",
    "            else:  # gpt\n",
    "                conversation_text += f\"ASSISTANT: {conv['value']}\\n\"\n",
    "        \n",
    "        # Tokenize\n",
    "        # For training, we want input_ids to be the full conversation\n",
    "        # and labels to mask out the user parts (only train on assistant responses)\n",
    "        full_text = conversation_text\n",
    "        \n",
    "        # Tokenize the full conversation\n",
    "        encoded = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'][0]\n",
    "        attention_mask = encoded['attention_mask'][0]\n",
    "        \n",
    "        # Create labels (mask USER parts, only train on ASSISTANT parts)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Find ASSISTANT token positions\n",
    "        assistant_token = self.tokenizer.encode(\"ASSISTANT:\", add_special_tokens=False)\n",
    "        user_token = self.tokenizer.encode(\"USER:\", add_special_tokens=False)\n",
    "        \n",
    "        # Simple approach: mask everything except what comes after \"ASSISTANT:\"\n",
    "        # More sophisticated version would track exact positions\n",
    "        labels_list = labels.tolist()\n",
    "        \n",
    "        # Mask all positions initially\n",
    "        for i in range(len(labels_list)):\n",
    "            labels_list[i] = -100\n",
    "        \n",
    "        # Find and unmask assistant responses\n",
    "        text_ids = input_ids.tolist()\n",
    "        i = 0\n",
    "        while i < len(text_ids):\n",
    "            # Look for \"ASSISTANT:\" pattern\n",
    "            if i + len(assistant_token) <= len(text_ids):\n",
    "                if text_ids[i:i+len(assistant_token)] == assistant_token:\n",
    "                    # Found ASSISTANT, unmask from here until next USER or end\n",
    "                    j = i + len(assistant_token)\n",
    "                    while j < len(text_ids):\n",
    "                        # Check if we hit USER token\n",
    "                        if j + len(user_token) <= len(text_ids):\n",
    "                            if text_ids[j:j+len(user_token)] == user_token:\n",
    "                                break\n",
    "                        # Check if we hit padding\n",
    "                        if text_ids[j] == self.tokenizer.pad_token_id:\n",
    "                            break\n",
    "                        labels_list[j] = text_ids[j]\n",
    "                        j += 1\n",
    "                    i = j\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        labels = torch.tensor(labels_list)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "print(\"Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T12:11:05.017608Z",
     "iopub.status.busy": "2026-01-16T12:11:05.017233Z",
     "iopub.status.idle": "2026-01-16T12:11:18.663511Z",
     "shell.execute_reply": "2026-01-16T12:11:18.662458Z",
     "shell.execute_reply.started": "2026-01-16T12:11:05.017573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 157712 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 29571\n",
      "Warmup steps: 100\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/78856 [00:00<?, ?it/s]c:\\Users\\DzungTa\\anaconda3\\envs\\vlm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Initialize Model and Dataset\n",
    "print(\"Initializing model...\")\n",
    "\n",
    "# Use TinyLlama - it's open and doesn't require authentication\n",
    "model = LLaVAModel(\n",
    "    vision_model_name=\"openai/clip-vit-large-patch14\",\n",
    "    llm_model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    projection_dim=2048\n",
    ")\n",
    "\n",
    "train_dataset = LLaVADataset(\n",
    "    jsonl_path = \"./llava_train.jsonl\",\n",
    "    vision_processor=model.vision_processor,\n",
    "    tokenizer=model.tokenizer,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Resize token embeddings if we added new tokens\n",
    "model.llm.resize_token_embeddings(len(model.tokenizer))\n",
    "model = model.to(device)\n",
    "\n",
    "# Cell 6: Training Configuration\n",
    "BATCH_SIZE = 2  # Small batch size for memory constraints\n",
    "GRAD_ACCUMULATION_STEPS = 8  # Effective batch size = 2 * 8 = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_STEPS = 100\n",
    "SAVE_STEPS = 500\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Only train projection layer and optionally LoRA for LLM\n",
    "trainable_params = list(model.mm_projector.parameters())\n",
    "\n",
    "# Option: Add LoRA to LLM for parameter-efficient fine-tuning\n",
    "# This would require peft library - uncomment if you want to use it\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "# model.llm = get_peft_model(model.llm, lora_config)\n",
    "# trainable_params.extend(model.llm.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS // GRAD_ACCUMULATION_STEPS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {WARMUP_STEPS}\")\n",
    "\n",
    "# Cell 7: Training Loop\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, epoch, grad_accum_steps):\n",
    "    model.train()\n",
    "    # Keep vision tower in eval mode\n",
    "    model.vision_tower.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            images=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / grad_accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item() * grad_accum_steps,\n",
    "            'lr': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (step + 1) % SAVE_STEPS == 0:\n",
    "            checkpoint_path = f\"./llava_checkpoint_epoch{epoch+1}_step{step+1}.pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'step': step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': total_loss / (step + 1),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"\\nCheckpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Cell 8: Run Training\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        GRAD_ACCUMULATION_STEPS\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} - Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save epoch checkpoint\n",
    "    checkpoint_path = f\"./llava_epoch{epoch+1}_final.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': avg_loss,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Epoch checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = \"model/llava_final_model.pt\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 31296,
     "sourceId": 39911,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1573501,
     "sourceId": 2598787,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8866651,
     "sourceId": 13915433,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
